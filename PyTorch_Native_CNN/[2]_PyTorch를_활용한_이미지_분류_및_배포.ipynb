{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch를 활용한 이미지 분류 및 배포"
      ],
      "metadata": {
        "id": "_kR0IXfCeuV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nvidia-GPU 설정 여부 확인"
      ],
      "metadata": {
        "id": "1fT9gclbe5w6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nej_coy-eeyT"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 쓰레기 분류 데이터셋을 활용한 이미지 분류"
      ],
      "metadata": {
        "id": "Ib_bIm1ue9AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 종류의 쓰레기 사진을 촬영한 데이터셋으로 이미지 분류 모델을 훈련하고, 이를 활용한 쓰레기 분류 프로그램을 제작합니다."
      ],
      "metadata": {
        "id": "hhyhkqRXfBju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 클래스"
      ],
      "metadata": {
        "id": "zfBzaxlufSHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 분류를 위한 데이터셋을 다운로드 받은 후 이를 사용합니다.\n",
        "\n",
        "데이터셋 위치: https://github.com/JinFree/Recycle_Classification_Dataset.git"
      ],
      "metadata": {
        "id": "-H0g_ffwffp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "class PyTorch_Classification_Dataset_Class(Dataset):\n",
        "    def __init__(self\n",
        "                , dataset_dir = \"/content/Recycle_Classification_Dataset\"\n",
        "                , transform = None):\n",
        "        super().__init__()\n",
        "        if not os.path.isdir(dataset_dir):\n",
        "            os.system(\"git clone https://github.com/JinFree/Recycle_Classification_Dataset.git\")\n",
        "            os.system(\"rm -rf ./Recycle_Classification_Dataset/.git\")\n",
        "        self.image_abs_path = dataset_dir\n",
        "        self.transform = transform\n",
        "        if self.transform is None:\n",
        "            self.transform = transforms.Compose([\n",
        "                    transforms.Resize(256)\n",
        "                    , transforms.RandomCrop(224)\n",
        "                    , transforms.ToTensor()\n",
        "                    , transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "        self.label_list = os.listdir(self.image_abs_path)\n",
        "        self.label_list.sort()\n",
        "        self.x_list = []\n",
        "        self.y_list = []\n",
        "        for label_index, label_str in enumerate(self.label_list):\n",
        "            img_path = os.path.join(self.image_abs_path, label_str)\n",
        "            img_list = os.listdir(img_path)\n",
        "            for img in img_list:\n",
        "                self.x_list.append(os.path.join(img_path, img))\n",
        "                self.y_list.append(label_index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.x_list[idx])\n",
        "        if image.mode is not \"RGB\":\n",
        "            image = image.convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, self.y_list[idx]\n",
        "\n",
        "    def __save_label_map__(self, dst_text_path = \"label_map.txt\"):\n",
        "        label_list = self.label_list\n",
        "        f = open(dst_text_path, 'w')\n",
        "        for i in range(len(label_list)):\n",
        "            f.write(label_list[i]+'\\n')\n",
        "        f.close()\n",
        "\n",
        "    def __num_classes__(self):\n",
        "        return len(self.label_list)"
      ],
      "metadata": {
        "id": "kkB8tHH3fAz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이미지 분류 모델 클래스"
      ],
      "metadata": {
        "id": "ebo2aLBqfdAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "빠른 수렴을 위해 MobileNet V2의 pre-trained weights로부터 마지막 Softmax 레이어만 바꿔서 훈련을 수행합니다."
      ],
      "metadata": {
        "id": "gdiUjaVcfx0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model_Class_Transfer_Learning_MobileNet(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.network = models.mobilenet_v2(pretrained=pretrained)\n",
        "        num_ftrs = self.network.classifier[1].in_features\n",
        "        self.network.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "        self.classifier = nn.Sequential(nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.network(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "94HPfuEKfemy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 훈련을 위한 코드 전반적으로 구현한 클래스"
      ],
      "metadata": {
        "id": "Fs4liKr7gmKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class PyTorch_Classification_Training_Class():\n",
        "    def __init__(self\n",
        "                , dataset_dir = \"/content/Recycle_Classification_Dataset\"\n",
        "                , batch_size = 16\n",
        "                , train_ratio = 0.75\n",
        "                ):\n",
        "        if not os.path.isdir(dataset_dir):\n",
        "            os.system(\"git clone https://github.com/JinFree/Recycle_Classification_Dataset.git\")\n",
        "            os.system(\"rm -rf ./Recycle_Classification_Dataset/.git\")\n",
        "            dataset_dir = os.path.join(os.getcwd(), 'Recycle_Classification_Dataset')\n",
        "        self.USE_CUDA = torch.cuda.is_available()\n",
        "        self.DEVICE = torch.device(\"cuda\" if self.USE_CUDA else \"cpu\")\n",
        "        self.transform = transforms.Compose([\n",
        "                transforms.Resize(256)\n",
        "                , transforms.RandomCrop(224)\n",
        "                , transforms.ToTensor()\n",
        "                , transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "                ])\n",
        "        dataset = PyTorch_Classification_Dataset_Class(dataset_dir = dataset_dir, transform = self.transform)\n",
        "        dataset.__save_label_map__()\n",
        "        self.num_classes = dataset.__num_classes__()\n",
        "        train_size = int(train_ratio * len(dataset))\n",
        "        test_size = len(dataset) - train_size\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset\n",
        "            , batch_size=batch_size\n",
        "            , shuffle=True\n",
        "        )\n",
        "        self.test_loader = torch.utils.data.DataLoader(\n",
        "            test_dataset\n",
        "            , batch_size=batch_size\n",
        "            , shuffle=False\n",
        "        )\n",
        "        self.model = None\n",
        "        self.model_str = None\n",
        "\n",
        "    def prepare_network(self):\n",
        "        self.model = Model_Class_Transfer_Learning_MobileNet (self.num_classes)\n",
        "        self.model_str = \"PyTorch_Transfer_Learning_MobileNet\"\n",
        "        self.model.to(self.DEVICE)\n",
        "        self.model_str += \".pt\"\n",
        "\n",
        "    def training_network(self\n",
        "            , learning_rate = 0.0001\n",
        "            , epochs = 10\n",
        "            , step_size = 3\n",
        "            , gamma = 0.3):\n",
        "        if self.model is None:\n",
        "            self.prepare_network(False)\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "        acc = 0.0\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            self.model.train()\n",
        "            for data, target in tqdm(self.train_loader):\n",
        "                data, target = data.to(self.DEVICE), target.to(self.DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = F.cross_entropy(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            scheduler.step()\n",
        "            self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in tqdm(self.test_loader):\n",
        "                    data, target = data.to(self.DEVICE), target.to(self.DEVICE)\n",
        "                    output = self.model(data)\n",
        "                    test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "                    pred = output.max(1, keepdim=True)[1]\n",
        "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            test_loss /= len(self.test_loader.dataset)\n",
        "            test_accuracy = 100. * correct / len(self.test_loader.dataset)\n",
        "            print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch, test_loss, test_accuracy))\n",
        "            if acc < test_accuracy or epoch == epochs:\n",
        "                acc = test_accuracy\n",
        "                torch.save(self.model.state_dict(), self.model_str)\n",
        "                print(\"model saved!\")"
      ],
      "metadata": {
        "id": "jxXeiMcJgqnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련"
      ],
      "metadata": {
        "id": "X5QapL8HhGIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 위한 데이터셋을 다운로드받고 모델을 준비합니다.\n",
        "\n",
        "약 7분 정도의 시간이 필요합니다."
      ],
      "metadata": {
        "id": "hEDkaTP6hd4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jetsonai/DeepLearning4Projects/raw/refs/heads/main/Chap5/test_video.mp4"
      ],
      "metadata": {
        "id": "pWgd5bEQhoYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/Recycle_Classification_Dataset\"\n",
        "batch_size = 64\n",
        "train_ratio = 0.75\n",
        "training_class = PyTorch_Classification_Training_Class(dataset_dir = dataset_dir\n",
        "                                                        , batch_size = batch_size\n",
        "                                                        , train_ratio = train_ratio)\n",
        "training_class.prepare_network()"
      ],
      "metadata": {
        "id": "n1H1I4UphKV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련을 수행합니다.\n",
        "\n",
        "T4 GPU를 사용할 떄 한 epoch에 약 5분이 필요합니다."
      ],
      "metadata": {
        "id": "Ww_oYlwah3T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_class.training_network(learning_rate = 0.00001, epochs=10, step_size=3, gamma=0.3)"
      ],
      "metadata": {
        "id": "tR_PXFGghplo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 학습이 끝나면 label_map.txt와 PyTorch_Transfer_Learning_MobileNet.pt 파일을 다운로드할 수 있으며, 배포에는 이 파일이 필요합니다."
      ],
      "metadata": {
        "id": "eZsIfTljh9I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 배포"
      ],
      "metadata": {
        "id": "rD7BQ-5EiG85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지 분류 모델을 훈련한 후 배포할 때는 모델의 구조와 가중치, 각 출력이 어떤 클래스에 매핑되는지 정보가 필요합니다.\n",
        "\n",
        "위 코드 중 Model_Class_Transfer_Learning_MobileNet 클래스는 모델의 구조이며, PyTorch_Transfer_Learning_MobileNet.pt 는 가중치입니다. label_map.txt은 모델의 출력이 어떤 클래스에 매핑되는지 정보입니다.\n",
        "\n",
        "이를 토대로 추론 코드를 작성합니다."
      ],
      "metadata": {
        "id": "uLJdLttciKqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "class Inference_Class():\n",
        "    def __init__(self):\n",
        "        USE_CUDA = torch.cuda.is_available()\n",
        "        self.DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "        self.model = None\n",
        "        self.label_map = None\n",
        "        self.transform_info = transforms.Compose([\n",
        "                transforms.Resize(size=(224, 224)),\n",
        "                transforms.ToTensor()\n",
        "                ])\n",
        "\n",
        "\n",
        "    def load_model(self, label_map_file = \"label_map.txt\"):\n",
        "        self.label_map = np.loadtxt(label_map_file, str, delimiter='\\t')\n",
        "        num_classes = len(self.label_map)\n",
        "        self.model = Model_Class_Transfer_Learning_MobileNet(num_classes).to(self.DEVICE)\n",
        "        model_str = \"PyTorch_Transfer_Learning_MobileNet\"\n",
        "        model_str += \".pt\"\n",
        "        self.model.load_state_dict(torch.load(model_str, map_location=self.DEVICE))\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "    def inference_video(self, video_source=\"test_video.mp4\", save_video=\"result_video.avi\"):\n",
        "        cap = cv2.VideoCapture(video_source)\n",
        "        if cap.isOpened():\n",
        "            print(\"Video Opened\")\n",
        "        else:\n",
        "            print(\"Video Not Opened\")\n",
        "            print(\"Program Abort\")\n",
        "            exit()\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out = None\n",
        "        if save_video is not None:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'XVID') #.avi\n",
        "            out = cv2.VideoWriter(save_video, fourcc, fps, (width, height), True)\n",
        "        with torch.no_grad():\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if ret:\n",
        "                    output = self.inference_frame(frame)\n",
        "                    if out is not None:\n",
        "                        out.write(output)\n",
        "                else:\n",
        "                    break\n",
        "            cap.release()\n",
        "            if out is not None:\n",
        "                out.release()\n",
        "        return\n",
        "\n",
        "\n",
        "    def inference_frame(self, opencv_frame):\n",
        "        opencv_rgb = cv2.cvtColor(opencv_frame, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(opencv_rgb)\n",
        "        image_tensor = self.transform_info(image)\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "        image_tensor = image_tensor.to(self.DEVICE)\n",
        "        inference_result = self.model(image_tensor)\n",
        "        inference_result = inference_result.squeeze()\n",
        "        inference_result = inference_result.cpu().numpy()\n",
        "        result_frame = np.copy(opencv_frame)\n",
        "        label_text = self.label_map[np.argmax(inference_result)]\n",
        "        label_text += \" \" + str(inference_result[np.argmax(inference_result)])\n",
        "        result_frame = cv2.putText(result_frame, label_text, (10, 50), cv2.FONT_HERSHEY_PLAIN, fontScale=2.0, color=(0,0,255), thickness=3)\n",
        "        return result_frame"
      ],
      "metadata": {
        "id": "FaLjjVAfiGU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "추론을 수행합니다."
      ],
      "metadata": {
        "id": "OazzxV8njldO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inferenceClass = Inference_Class()\n",
        "inferenceClass.load_model()\n",
        "inferenceClass.inference_video(\"test_video.mp4\", \"result_video.avi\")"
      ],
      "metadata": {
        "id": "rV-PymRcjWQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 코드를 잘 수정하면 카메라에서 실시간으로 프레임을 받아오면서 이미지 분류를 수행하고, 그 결과에 따라 후처리를 수행할 수 있습니다."
      ],
      "metadata": {
        "id": "LAFYUWrSjVxq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73Ju4YIDjtQA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}